# Regression-from-Scratch
This repository implements **Regression algorithms from scratch** using only Python and its fundamental libraries (e.g., `numpy`, `pandas`). The goal is to understand the inner workings of regression models by building them step by step without relying on machine learning frameworks like scikit-learn.

## ğŸ“Œ Features
- Implementation of **Linear Regression** (univariate and multivariate).
- Support for both **Batch Gradient Descent** and **Normal Equation** methods.
- Option to use **Stochastic Gradient Descent (SGD)** for faster training on large datasets.
- Cost function (Mean Squared Error) visualization during training.
- Comparison with scikit-learnâ€™s regression results.

## ğŸ§® Mathematics Behind Regression
### 1. Hypothesis Function
Linear regression assumes a linear relationship between input features **X** and target **y**:
Å· = hÎ¸(X) = Î¸0 + Î¸1x1 + Î¸2x2 + ... + Î¸nxn
where:
- Å· â†’ predicted output  
- Î¸ â†’ model parameters (weights)  
- xi â†’ input features  

### 2. Cost Function (Mean Squared Error)
The cost function measures how well our model fits the data:
J(Î¸) = (1 / 2m) Î£ ( hÎ¸(xá¶¦) - yá¶¦ )Â²
where:
- m â†’ number of training examples  
- hÎ¸(xá¶¦) â†’ predicted value  
- yá¶¦ â†’ actual value  

### 3. Gradient Descent Optimization
We minimize the cost function using **gradient descent**:
Î¸j := Î¸j - Î± * âˆ‚J(Î¸) / âˆ‚Î¸j
where:
- Î± â†’ learning rate  
- âˆ‚J(Î¸)/âˆ‚Î¸j â†’ gradient of the cost function  

For linear regression, the gradient is: âˆ‚J(Î¸) / âˆ‚Î¸j = (1/m) Î£ ( hÎ¸(xá¶¦) - yá¶¦ ) * xjá¶¦

### 4. Normal Equation (Analytical Solution)
Instead of gradient descent, parameters can be directly computed:
Î¸ = (Xáµ€X)â»Â¹ Xáµ€y
where:
- X â†’ input feature matrix (with bias term)  
- y â†’ target vector  
This method works well for small datasets but becomes computationally expensive for very large feature sets.


## ğŸ¤ Contributing
Contributions are welcome! Feel free to fork the repository, open issues, and submit pull requests.






